---
title: "Regression is all you need"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Antes de empezar...
Instala las librerías necesarias (copia y pega en la terminal; no descomentes
la línea)...

```{r}
# install.packages(
#  c("easystats", "GGally", "qqplotr")
# )
```

... y carga las librerías más usadas: 

```{r, message=FALSE, warning=FALSE}
library("easystats")
library("tidyverse")
library("readr")
theme_set(theme_bw())  # cambia el tema de ggplot
```

El modelado estadístico es difícil, por lo que en este notebook solo 
cubriremos algunos aspectos básicos. Si en el futuro te enfrentas a experimentos
complejos, considera buscar ayuda.

>To consult the statistician after an experiment is finished is often merely to 
ask him to conduct a post mortem examination. He can perhaps say what the 
experiment died of. (Ronald Fisher)

## Modelado estadístico: las dos culturas
En el artículo clásico [Statistical modeling: The two cultures]((https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full)) encontramos 

> There are two cultures in the use of statistical modeling to reach conclusions from
data. One assumes that the data are generated by a given stochastic data model. The
other uses algorithmic models and treats the data mechanism as unknown. (Leo Breiman)

* *Explanatory modeling*: modelos estadísticos empleados para probar una teoría.
Para ello se emplean variables con un significado científico claro y se evalúa si tienen 
una relación *significativa* con la variable de interés. $\rightarrow$ "Estadística clásica"
basada en **modelos lineales**.
* *Predictive modeling*: modelos cuyo propósito es predecir observaciones nuevas 
o futuras. La interpretación de los modelos es secundaria. 
Gran parte del $\rightarrow$ **Aprendizaje automático** (*machine learning*) se 
basa en esta perspectiva.


## Regresión simple

El modelo básico sobre el que se construye gran parte de la estadística
es el modelo de **regresión lineal**:
$$y = a + b\cdot x + \epsilon$$
donde $\epsilon \sim \mathcal{N}(0, \sigma^2)$.

Es instructivo simular datos que sigan este modelo para entender el significado
de la ecuación. Para facilitar la interpretación, asumamos que estamos estudiando
la altura de los niños (de 0 a 12 años) en función de su edad:

$$altura = a + b\cdot edad + \epsilon \qquad \qquad \text{(altura en cm y edad en años)}$$

```{r}
age = seq(0, 12, 0.1)

# No todos los niños de la misma edad miden lo mismo. El término a + b * edad
# debe interpretarse como una altura media...
mean_height = 50 + 6.5 * age 
# ... en torno a la cuál habrá fluctuaciones estadísticas que podemos asumir normales.
epsilon = rnorm(length(mean_height), sd = 5)

height = mean_height + epsilon

df = data.frame(
  'age' = age,
  'height' = height, 
  'mean_height' = mean_height
)

ggplot(df, aes(x = age, y = height)) + 
  geom_point() + 
  geom_line(aes(y = mean_height), col = 2)

    # OR...
# plot(x, y)
# lines(x, expected_behaviour, col = 2)
```

En general, en una regresión lineal 
$$y = a + b\cdot x + \epsilon$$
la  relación entre $x$ e $y$ es muy específica. Si aumenta (disminuye) $x$ aumenta
(disminuye) $y$. El hecho de que $y$ aumente o disminuye en función de $x$ depende 
del signo de $a$. Además, un  incremento de una unidad en $x$ siempre produce el 
mismo incremento en $y$ (ídem si $x$ disminuye). Se dice que la relación entre 
$x$ e $y$ es **lineal**.

La primera pregunta a la que nos enfrentamos es la siguiente. 
**Dados los datos $(x, y)$, ¿podemos estimar el `comportamiento medio` (`expected age` en nuestro ejemplo)?**

![](https://media.giphy.com/media/l0ErOholJjSmFlMFG/giphy.gif)

### Ejemplo: linear model (lm)
```{r}
# 1) crear un modelo lineal... Interpretaremos la fórmula del modelo líneal como
# una forma de preguntar se "la altura depende de la edad".
naive_model = lm(height ~ age, data = df)
# 2) obtener estimaciones de a y b
summary(naive_model)
# 3) Obtener predicciones del modelo lineal
preds = predict(naive_model, interval = "confidence")
# 4) visualizar el ajuste
data_and_preds = bind_cols(df, preds)
ggplot(data_and_preds, aes(x = age)) + 
  geom_point(aes(y = height)) + 
  geom_line(aes(y = mean_height, col = "Expected")) + 
  geom_line(aes(y = fit, col = "Predicted")) + 
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.2, fill = "black")
```

Dado que las estimaciones incorporan el error de estimación, es posible realizar
inferencia acerca de la **significación** de los parámetros.


### Ejercicio: ggplot y lm
Si solo nos interesa la visualización y no los valores de los coeficientes podemos
emplear `geom_smooth(method = "lm")` para pintar la recta de regresión. Aplícalo a los
datos anteriores.

```{r}
ggplot(df, aes(x = age, y = height)) + 
  geom_point() # ???????????? Añade geom_smooth
```


### Ejemplo: inferencia con lm
Un estudiante de biología desea determinar la relación entre
temperatura ambiente y frecuencia cardíaca en la rana leopardo, *Rana pipiens*.
Para ello, manipula la temperatura en incrementos de 2ºC que van desde
2ºC a 18ºC, registrando la frecuencia cardíaca (pulsaciones por minuto) en cada
intervalo. Los datos están disponibles en "hr.csv".

```{r}
# 1) leer los datos
#MY CODE
  hr <- read.csv("tema2/data/hr.csv",sep = " ")
  ts <- seq(2,18,2)
  #scatterplot
  ggplot(hr,aes(x = temperature,y = heart_rate)) + geom_point()+geom_smooth(method = "lm")
# 2) Crear un modelo lineal
 frog_model = lm(heart_rate ~ temperature,hr)
# 3) Inferencia
 summary(frog_model)
 #heart rate = 2.13 + 1.77 * temperature
 
 #predicciones
 pred <- predict(frog_model,interval = "confidence")
 data_and_preds = bind_cols(hr, pred)
ggplot(data_and_preds, aes(x = temperature)) + 
  geom_point(aes(y = heart_rate)) + 
  geom_line(aes(y = fit, col = "Predicted")) + 
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.2, fill = "black")
#los datos aportan evidencia que apoya que existe una relacion entre el hr y la temperatura (p-valor = 1.63*e-5)
#la relacion es positiva: a mayor temperatura mayor hr. por cada grado celsius extra el ritmo cardiaco
#aumenta 1.77bpm en media
```

---

El diseño experimental y los resultados de la inferencia en el ejemplo de las ranas
nos invitan a concluir que "el aumento de la temperatura *causa* un incremento de 
la frecuencia cardíaca". Aunque en este caso esto es probablemente correcto, en general, 
esto no es así. Por muy fuerte que  parezca la relación entre las variables $x$ e $y$,
**no debemos interpretar  una variable como la causa de la otra**. Una relación
significativa entre $x$ e $y$ puede ocurrir por varios motivos:

1. $x$ causa $y$.
2. $y$ causa $x$.
3. Existe un tercer factor (llamado **variable de confusión**) que, bien directa
o indirectamente, causa $x$ e $y$.

![](https://qph.fs.quoracdn.net/main-qimg-13d22f6fda3811a9108d18b71c46e933-pjlq)

Para poder hacer afirmaciones acerca de la causalidad necesitamos muchos detalles
acerca del **diseño experimental**. En general, en nuestros ejercicios no dispondremos
de estos datos, por lo que lo mejor es ser cautelosos.

Por otra parte, respecto a los **p-valores**...

> There is some debate among statisticians and researchers about the appropriateness 
of P values, and that the term "statistical significance" can be misleading. If you 
have a small P value, it only means that the effect being tested is unlikely to be
explained by chance variation alone, in the context of the current study and the 
current statistical model underlying the test. If you have a large P value, it 
only means that the observed effect could plausibly be due to chance alone: it
is wrong to conclude that there is no effect (emmeans package authors)

En general, hay consenso en que 
**debe abandonarse la interpretación dicotómica del p-valor** 
(efecto significativo Vs no-significativo, sobre todo teniendo en 
cuenta que se basan en un threshold arbitrario) y 
**que debe favorecerse los resultados basados en intervalos de confianza y tamaños de los efectos**
(¡incluso si no son significativos!)

> For example, a study on the effects of two different ambient temperatures on 
paramecium diameter returning an effect size of 20 µm and a p-value of 0.1, 
if centred on p-value interpretation would conclude 'no effect' of temperature,
despite the best supported effect size being 20, not 0. An interpretation based on effect size and confidence intervals could, for example, state: 'Our results suggest that 
paramecium kept at the lower temperature will be on average 20 µm larger in size, 
however a difference in size ranging between −4 and 50 µm is also reasonably likely'. 
(...), the latter approach acknowledges the uncertainty in the estimated effect 
size while also ensuring that you do not make a false claim either of no effect
if p > 0.05, or an overly confident claim. (Lewis G. Halsey, [The reign of p-value is over](https://royalsocietypublishing.org/doi/10.1098/rsbl.2019.0174))

### Ejemplo: intervalos de confianza en lm
```{r}
summary(frog_model)
confint(frog_model)
```
Los resultados sugieren que el ritmo_cardiaco de las ranas se incrementará
1.77 bpms por cada grado Celsius, si bien un aumento de la frecuencia 
cardiaca en el rango (1.37, 2.17) es igualmente verosímil.

### Validación de los modelos
Evidentemente cualquier interpretación está supeditada a que el modelo sea 
correcto. Debemos ser muy cuidadosos a la hora de verificar que se cumplan las
asunciones del modelo de regresión lineal. Podemos usar el acrónimo **LINE** para
recordar las asunciones más importantes del modelo: 
**Linear, Independent, Normal, Equal variances**.

### Ejemplo: evaluación del modelo `naive_model`
```{r}
# Lo primero siempre debe ser pintar las predicciones (por ejemplo, con geom_smooth)
# para ver si se ajustan de forma razonable a los datos. Como ya lo hemos hecho, 
# lo saltamos...

# Para QQplot también nos apoyaremos en check_normality
plot(naive_model, ask = F) 

# Comprobar la normalidad con qqplot puede ser difícil. Podemos apoyarnos en 
performance::check_normality(naive_model)

# check_normality corre shapiro.test, pero tal y como resalta la documentación
# "this formal test almost always yields significant results for the distribution
# of residuals and visual inspection (e.g. Q-Q plots) are preferable."
is_norm = check_normality(naive_model)

# Para hacer la inspección visual
plot(is_norm) # probar con TRUE y FALSE
```

## Transformaciones logarítmicas
### Ejemplo: estudio y rendimiento
Se desea estudiar la relación entre el tiempo de estudio y el rendimiento en 
un examen particular. Datos en "exam_scores.csv".

```{r}
scores = read_csv("tema2/data/exam_scores.csv")
ggplot(scores, aes(x = study_time_hours, y = exam_score)) + 
  geom_point() + geom_smooth()
```

Nótese queme un aunto en el tiempo de estudio tiene un impacto sustancial en
el rendimiento inicial, pero este impacto disminuye a medida que aumenta el 
tiempo de estudio. Lo más reseñable para nuestro análisis es que la relación entre
ambas variables es ¡NO-LINEAL! 

**Las transformaciones de variables nos pueden ayudar en estos casos**. La más típica
es la **transformación logarítmica, que  ayuda a linearizar relaciones**. Algunos 
casos habituales donde se suele emplear el logaritmo son:

* Con variables donde se espera un efecto multiplicativo y/o rendimientos crecientes/decrecientes.
* Con variables donde es más relevante el orden de magnitud que el valor numérico preciso.

```{r}
#logaritmo se aplica al que genera diminishing return, no al diminishing return
ggplot(scores, aes(x = log(study_time_hours), y = exam_score)) + 
  geom_point() + geom_smooth(method = "lm")

# o mejor...
ggplot(scores, aes(x = study_time_hours, y = exam_score)) + 
  geom_point() + 
  coord_trans(x = "log") + geom_smooth(method = "lm")
```

```{r}
study_model = lm(exam_score ~ log(study_time_hours), scores)
summary(study_model)
```

¡Ojo! Con este truco, cambia la interpretación de los coeficientes de la regresión.
Usa `log_interpretation` de `utils.R`:

```{r}
suppressPackageStartupMessages(
  source("finalprep/utils.R")
)
log_interpretation(20.2753, "predictor", percent_increase = 20)
```


### Ejercicio: ¿Influye la altura en los salarios?
Quizás hayas escuchado que la gente más alta gana más. ¿Es cierto? Usa los datos
en "height_earnings.csv" para indigar sobre esto (datos procedentes de una encuesta
sobre trabajo, familia y bienestar de EEUU; las alturas están en pulgadas y las 
ganancias son ganancias anuales en dolares). Usa la transformación logarítmica
sobre las ganancias para centrarte en el orden de magnitud.

```{r}
earns <- read.csv("tema2/data/heights_earnings.csv")

#Visualizar datos
ggplot(earns,aes(x = height, y = log(earn))) + geom_point()+geom_smooth(method = "lm")
ggplot(earns,aes(x = height, y = log(earn)))  + geom_jitter(width = .5, alpha = 0.4) + geom_smooth(method = "lm")

#Crear el modelo
study_model = lm(log(earn) ~ (height), earns)

#Comprobar las asuunciones del modelo
plot(study_model, ask  = F)

isnorm <- performance::check_normality(study_model)
plot(isnorm, type = "qq")

#Summary y conclusiones
summary(study_model)

#el pvalor es 0: se confirma que ser más alto aumenta el salario.


log_interpretation(0.04273,"response",1)
#Por cada centimetro el salario aumenta 4.36%

# Resultado esperado: cada cm extra parece incrementar el salario en 4.36%
```


Las conclusiones de este ejercicio seguramente no sean correctas, ya que estamos
intentando explicar el salario empleando un solo factor (altura) cuando en realidad
el salario está determinado por múltiples factores. ¿Se te ocurre algún ejemplo 
de por qué nuestras conclusiones pueden estar sesgadas? (Intenta pensar en un colectivo
que suele ganar menos dinero y que también suele tener alturas menores.)


## Regresión múltiple
Para lidiar con situaciones como la ilustrada en el gráfico de 
"correlation is not causation" (tiburones Vs helados)
necesitamos emplear modelos de **regresión múltiple**, dado que estos permiten 
"controlar" las variables de confusión. Crear un modelo de regresión múltiple
es análogo al caso unidimensional...

### Ejemplo: ¿Qué influye en los salarios?
Añade la edad a tu modelo de los salarios para mejorar las predicciones.

```{r}
library("GGally")
source("tema2/utils.R")

df = read.csv("tema2/data/heights_earnings.csv")
ggpairs(df[, c("earn", "height", "age")])


ha_model = lm(log(earn) ~ height + age, df)
df$predictions = predict(ha_model)

summary(ha_model)

#log(earn) = 7.02 + 0.04 * height + 0.0006 + age
#pvalor de age = 0.591, por lo que no afecta al salario
log_interpretation(0.0430069,"response")
#PARA PERSONAS DE LA MISMA EDAD cada cm extra incrementa el salario en 4.39%  

```

### Ejercicio: asunciones del modelo de los salarios

```{r}
# ??
```

---

Hasta ahora solo hemos usado datos continuos, pero nada evita **usar datos categóricos
como predictores **. ¡Ojo! Los coeficientes asociados a datos categóricos** no deben 
interpretarse como una pendiente**.

### Ejemplo: regresión múltiple con datos categóricos
Construye un modelo de regresión lineal para predecir el peso de una persona a partir
de los datos contenidos en "antrop.csv". Interpreta los coeficientes de la regresión.

```{r}
antrop = read.csv("tema2/data/antrop.csv")

antrop$male <- factor(antrop$male)

ggplot(antrop,aes(x = height, y = weight, col = male)) + geom_point() + geom_smooth(method = "lm")

#
antrop_model = lm(weight~height+male, data = antrop)

# Chequea las asunciones del modelo.
isnorm <- performance::check_normality(antrop_model)
plot(isnorm,type = "qq")

# Descomenta las siguientes líneas tras completar las líneas anteriores
antrop_preds = bind_cols(antrop, fit = predict(antrop_model))
ggplot(antrop_preds, aes(x = height, col=male)) + 
   geom_point(aes(y = weight)) + 
   geom_line(aes(y = fit), lwd = 3)

 summary(antrop_model)
```

El modelo se puede escribir como
$$weight = -29 + 0.47 * height + 1.23 * male$$ 
por lo que 1.23 significa que, de media y para una misma altura, los hombres 
pesan 1.23 Kg más que las mujeres (hemos **ajustado por el efecto de la altura**).
Esto es muy importante: **a la hora de interpretar un coeficiente en regresión múltiple
lo hacemos asumiendo "igualdad de condiciones" en los otros coeficientes.**

### Ejercicio: Intervalos de confianza
Usa intervalos de confianza para interpretar los resultados de la regresión.

```{r}
# ??
```

### Ejercicio: Howell
Los datos contenidos en "howell1.csv" son datos censales parciales del 
área !Kung San compilados a partir de entrevistas realizadas a finales de la década
de 1960. Crea un modelo para predecir el peso de los individuos a partir 
de la altura y el sexo. Evalúa la bondad del modelo.

```{r}
howell <- read.csv("tema2/data/howell1.csv", sep = ";")
hist(howell$weight)
# !!!!!!!!!!!  NO ES UNA DISTRIBUCION NORMAL !!!!!!!!!!!!!!!!!!
```

Sin ni siquiera usar `plot(howell_model)` ya somos capaces de ver que el ajuste
es malo... cualquier conclusión basada en un modelo erróneo será errónea 
(**garbage in, garbage out**).


### Ejemplo: dummy variables y contrastes
El dataset `iris` (puedes obtenerlo con `data(iris)`) contiene medidas del sépalo
y pétalo de varias especies de iris. Construye un modelo lineal para predecir
la longitud del sépalo únicamente en función de la especie. Interpreta los coeficientes
de la regresión.

```{r}
library("ggplot2")
data(iris)
iris_model = lm(Sepal.Length ~ Species, iris)
print(summary(iris_model))

iris_preds = bind_cols(iris, fit = predict(iris_model))
ggplot(iris_preds, aes(x=Species, fill = Species)) + 
  geom_boxplot(aes(y=Sepal.Length)) + 
  geom_point(aes(y = fit), shape=4, size=3)
```

Al interpretar los coeficientes de la regresión, observamos que 
`lm` ha tomado como referencia la especie `setosa`. Esto se puede observar usando
`contrasts`.

```{r}
contrasts(iris$Species)
```

Es decir el modelo es
$$sepal = \text{mean-setosa-sepal} + 0.93 * versicolor + 1.58 * virginica.$$

Sin embargo, podríamos reescribir el modelo de otra forma de forma 
que los coeficientes tengan otro significado. Un ejemplo sencillo sería:
$$sepal = \text{mean-versicolor-sepal} + \alpha_1 * setosa + \alpha_2 * virginica.$$

En este caso, simplimente estamos variando la especie de referencia. De hecho, 
`contrasts` se puede modificar para usar como referencia otro nivel del factor:

### Ejemplo: contrastes
```{r}
iris$Species = relevel(iris$Species, "versicolor")
contrasts(iris$Species)
iris_model_2 = lm(Sepal.Length ~ Species, iris)
print(summary(iris_model_2))
```

Lo interesante es que podemos 
**ajustar los contrastes de forma que respondan a nuestras preguntas científicas**. 
En general, estos contrastes deben ser **ortogonales**.

### Ejemplo: contrastes ortogonales
Imagínemonos el siguiente universo paralelo. En este universo paralelo solo 
existe la especie setosa. Una empresa de ingeniería genética te contrata para
crear nuevas especies con un sépalo más grande. Desarrollas un método conocido
como "Método V", que tiene dos variantes "V-I" y "V-II". Los experimentos con
estas variantes dan lugar a dos nuevas especies que llamas versicolor (V-I) y 
virginica (V-II). Te planteas dos preguntas científicas: 

1. ¿Es el método V capaz de crear especies con el sépalo más grande?
H0:1/2 mu vers + 1/2 mu virg = mu set  !!!!!!!!!!
Ha:1/2 mu vers + 1/2 mu virg != mu set !!!!!!!!!!
2. ¿Existe alguna diferencia entre V-I y V-II?
mu vers - mu virg != 0 !!!!!!!!!!!!!!!!!!!111

```{r}
suppressPackageStartupMessages(
  source("finalprep/utils.R")   # cargamos get_contrasts_coding
)
# Cambiamos otra vez la especie referencia (no es necesario, pero para tener 
# a las vs juntas)
iris$Species = relevel(iris$Species, "setosa")
levels(iris$Species)

contrasts(iris$Species)
#reescribir las formulas de antes en el orden en el que aparecen en esa operacion


#-mu,1/2,1/2 -> cambiado de orden y cogiendo las constantes:
#H0:1/2 mu vers + 1/2 mu virg = mu set
# la segunda linea se hace con muvers - muvirg != 0 (hay que reorganizar)
my_contrasts = rbind(
  #PONER NOMBRE PORQUE EL ORDEN DE LAS RESTAS IMPORTAN
  
  #mu - setosa
  "V - setosa" = c(-1, 0.5, 0.5),
  #versicolor - virginica
  "I - II" = c(0, 1, -1)
)
my_coding = get_contrasts_coding(my_contrasts)
contrasts(iris$Species) = my_coding
contrasts(iris$Species)
v_model = lm(Sepal.Length ~ Species, iris)
summary(v_model)
  confint(v_model)
```

El método V parace producir sépalos más grandes. Por otra parte, V-II es mejor
que V-I.

---

LLegados a este punto, ¡ya hemos cubierto el 90% de los contenidos habituales 
de un curso de estadística habitual! Aunque parezca mentira, ya hemos hecho 
análisis tan complejos como

* Análisis de la varianza (anova): por ejemplo, en el problema del método V,
* Análisis de la covarianza (ancova): con el dataset `antrop.csv` o `howell`,
* ...

Desde una perspectiva moderna, 
**todos los análisis clásicos (T-test, anova, ancova) pueden considerarse como simples modelos de regresión**.
Esto demuestra el  poder unificador de esta perspectiva. En las siguientes secciones
revisaremos sin embargo estos modelos clásicos para afianzar la conexión con los
modelos de regresión.


### Ejercicio: ¿Qué influye en los salarios?
Añade la variable sexo al modelo de los salarios. ¿Cuál es la diferencia en ganancias 
para hombres y mujeres de la misma altura? 

```{r}
earns <- read.csv("tema2/data/heights_earnings.csv")
earns$sex <- factor(earns$sex)
earns$ed <- factor(earns$ed)

#Visualizar datos


#Crear el modelo
study_model = lm(log(earn) ~ (height + sex), earns)

#Comprobar las asuunciones del modelo
plot(study_model, ask  = F)

isnorm <- performance::check_normality(study_model)
plot(isnorm, type = "qq")

#Summary y conclusiones
summary(study_model)


log_interpretation(0.018,"response")
log_interpretation(0.2661,"response")
#8.5 + 0.018 * height + 0.26 * male
#Hombres: 8.5 + 0.018 * height + 0.26
#Mujeres: 8.5 + 0.018 * height 


```

Añade la variable educación al modelo de los salarios. Modifica los contrastes
para responder a las siguientes preguntas:

1. ¿Merece la pena estudiar "high school" o "universidad" o basta con quedarse en
"elementary" (en términos de salario)?
  H0:1/2 m hs + 1/2 m u = m el
  Ha:1/2 m hs + 1/2 m u != m el
2. ¿Merece la pena estudiar "universidad" comparado con "high school"?
  m u - m hs != 0

```{r}
new_model <- lm(log(earn) ~ (height + sex + ed),earns)
summary(new_model)
#8.5 + 0.018 * height + 0.26 * male + 0.73 * uniorhigher
#Hombres: 8.5 + 0.018 * height + 0.26
#Mujeres: 8.5 + 0.018 * height 
#Elementary:8.5 + 0.018 * height + 0.26 * male
#Highschool:8.5 + 0.018 * height + 0.26 * male + 0.44
#Uni or Higher:8.5 + 0.018 * height + 0.26 * male + 0.73

contrasts(earns$ed)

my_contrasts <- rbind(
  " mu higher studies - el" = c(-1,0.5,0.5),
  " hs - uni" = c(0,1,-1)
)
my_coding = get_contrasts_coding(my_contrasts)
contrasts(earns$ed) = my_coding
contrasts(earns$ed)
v_model = lm(earn ~ ed, earns)
summary(v_model)
  confint(v_model)
# Es mejor estudiar highschool o uni que quedarse con la elemental. (se cobrarían de media 10360 euros más)
# Es aún mejor estudiar universidad que quedarse con highschool. (se cobrarían de media 7002 euros más con estudios universitarios)
```

